{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt1_P6h9qNaa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sys\n",
    "import argparse\n",
    "import csv\n",
    "from scipy.sparse import *\n",
    "import pandas as pd\n",
    "import regex \n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import f1_score,accuracy_score,precision_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUEUpiZiFdWp"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Concatenate,Conv1D,SpatialDropout1D,BatchNormalization,concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pbEdCxaSxO1"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BsKIFFjTSdD",
    "outputId": "da189351-b878-4043-d661-f89eea909bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
      "\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 10kB 19.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 20kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 30kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 40kB 15.5MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 3.6MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49717 sha256=30c4444227e9b9fbe61cbbbb1586f6ec10ff713181023e8969d57ad3437d5753\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCZqqMdtTT0D"
   },
   "outputs": [],
   "source": [
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3QUk5jYVQlh"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCnVjZ2HV2Kc"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTdWwlAoXJHz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDfGX4OZX5S8"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4yXt5sczMN_"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Permute, RepeatVector, Lambda\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, GRU, Bidirectional, Input, Multiply\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5dfXtC1AzpM"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DwzoMAGIZez"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "def load_w2v(filepath,binary):\n",
    "    return KeyedVectors.load_word2vec_format(filepath, binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0f7KDzFWC8"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):  \n",
    "    # ref: https://github.com/bakrianoo/aravec\n",
    "    search = [\"Ø£\",\"Ø¥\",\"Ø¢\",\"Ø©\",\"_\",\"-\",\"/\",\".\",\"ØŒ\",\" Ùˆ \",\" ÙŠØ§ \",'\"',\"Ù€\",\"'\",\"Ù‰\",\n",
    "              \"\\\\\",'\\n', '\\t','&quot;','?','ØŸ','!']\n",
    "    replace = [\"Ø§\",\"Ø§\",\"Ø§\",\"Ù‡\",\" \",\" \",\"\",\"\",\"\",\" Ùˆ\",\" ÙŠØ§\",\n",
    "               \"\",\"\",\"\",\"ÙŠ\",\"\",' ', ' ',' ',' ? ',' ØŸ ', ' ! ']\n",
    "    \n",
    "    tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(tashkeel,\"\", text)\n",
    "    \n",
    "    # longation = re.compile(r'(.)\\1+')\n",
    "    # subst = r\"\\1\\1\"\n",
    "    # text = re.sub(longation, subst, text)\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    text = re.sub(r\"[a-zA-Z]\", '', text)\n",
    "    text = re.sub(r\"\\d+\", ' ', text)\n",
    "    text = re.sub(r\"\\n+\", ' ', text)\n",
    "    text = re.sub(r\"\\t+\", ' ', text)\n",
    "    text = re.sub(r\"\\r+\", ' ', text)\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    text = text.replace('ÙˆÙˆ', 'Ùˆ')\n",
    "    text = text.replace('ÙŠÙŠ', 'ÙŠ')\n",
    "    text = text.replace('Ø§Ø§', 'Ø§')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2Bz6byOQIcr"
   },
   "outputs": [],
   "source": [
    "# Get the vec representation of a set of tweets based on a specified embedding (can be a word or emoji mapping)\n",
    "def get_tweets_embeddings(tweets, vec_map, embedding_dim=100, init_unk=False, variance=None, weighted_average=True):\n",
    "    # Get the variance of the embedding map\n",
    "    if init_unk and variance is None:\n",
    "        variance = embedding_variance(vec_map)\n",
    "        print(\"Vector mappings have variance \", variance)\n",
    "    # If set, calculate the tf-idf weight of each embedding, otherwise, no weighting (all weights are 1.0)\n",
    "    if weighted_average:\n",
    "        weights = get_tf_idf_weights(tweets, vec_map)\n",
    "    else:\n",
    "        weights = {k: 1.0 for k in vec_map.vocab}\n",
    "    tw_emb = np.zeros((len(tweets), embedding_dim))\n",
    "    for i, tw in enumerate(tweets):\n",
    "        total_valid = 0\n",
    "        for word in tw.split():\n",
    "            embedding_vector=None \n",
    "            if word in vec_map.wv:\n",
    "              embedding_vector = vec_map[word]\n",
    "            if embedding_vector is not None:\n",
    "                tw_emb[i] = tw_emb[i] + embedding_vector * weights[word]\n",
    "                total_valid += 1\n",
    "            elif init_unk:\n",
    "                seed(1337603)\n",
    "                tw_emb[i] = np.random.uniform(-variance, variance, size=(1, embedding_dim))\n",
    "            # else:\n",
    "            #    print(\"Not found: \", word)\n",
    "        # Get the average embedding representation for this tweet\n",
    "        tw_emb[i] /= float(max(total_valid, 1))\n",
    "    return tw_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oAYpQTxQVXL"
   },
   "outputs": [],
   "source": [
    "# Get the tf-idf weighting scheme (used to measure the contribution of a word in a tweet => weighted sum of embeddings)\n",
    "def get_tf_idf_weights(tweets, vec_map):\n",
    "    df = {}\n",
    "    for tw in tweets:\n",
    "        words = set(tw.split())\n",
    "        for word in words:\n",
    "            if word not in df:\n",
    "                df[word] = 0.0\n",
    "            df[word] += 1.0\n",
    "    idf = OrderedDict()\n",
    "    for word in vec_map.vocab:\n",
    "        n = 1.0\n",
    "        if word in df:\n",
    "            n += df[word]\n",
    "        score = math.log(len(tweets) / float(n))\n",
    "        idf[word] = score\n",
    "    return idf\n",
    "\n",
    "\n",
    "# Compute the similarity of 2 vectors, both of shape (n, )\n",
    "def cosine_similarity(u, v):\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.sqrt(np.sum(u ** 2))\n",
    "    norm_v = np.sqrt(np.sum(v ** 2))\n",
    "    cosine_distance = dot / (norm_u * norm_v)\n",
    "    return cosine_distance\n",
    "\n",
    "\n",
    "# Convert emojis to unicode representations by removing any variation selectors\n",
    "# Info: http://www.unicode.org/charts/PDF/UFE00.pdf\n",
    "def convert_emoji_to_unicode(emoji):\n",
    "    unicode_emoji = emoji.encode('unicode-escape')\n",
    "    find1 = unicode_emoji.find(b\"\\\\ufe0f\")\n",
    "    unicode_emoji = unicode_emoji[:find1] if find1 != -1 else unicode_emoji\n",
    "    find2 = unicode_emoji.find(b\"\\\\ufe0e\")\n",
    "    unicode_emoji = unicode_emoji[:find2] if find2 != -1 else unicode_emoji\n",
    "    return unicode_emoji\n",
    "\n",
    "\n",
    "# Performs the word analogy task: a is to b as c is to ____.\n",
    "def make_analogy(a, b, c, vec_map):\n",
    "    a = convert_emoji_to_unicode(a)\n",
    "    b = convert_emoji_to_unicode(b)\n",
    "    c = convert_emoji_to_unicode(c)\n",
    "\n",
    "    e_a, e_b, e_c = vec_map[a], vec_map[b], vec_map[c]\n",
    "\n",
    "    max_cosine_sim = -100\n",
    "    best = None\n",
    "    best_list = {}\n",
    "    for v in vec_map.keys():\n",
    "        # The best match shouldn't be one of the inputs, so pass on them.\n",
    "        if v in [a, b, c]:\n",
    "            continue\n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, vec_map[v] - e_c)\n",
    "        best_list[v] = cosine_sim\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best = v\n",
    "    sorted_keys = sorted(best_list, key=best_list.get, reverse=True)\n",
    "    print(\"Top 5 most similar emojis: \", [r.decode('unicode-escape') for r in sorted_keys[:5]])\n",
    "    print(str.format('{} - {} + {} = {}', a.decode('unicode-escape'), b.decode('unicode-escape'),\n",
    "                     c.decode('unicode-escape'), best.decode('unicode-escape')), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Get the Euclidean distance between two vectors\n",
    "def euclidean_distance(u_vector, v_vector):\n",
    "    distance = np.sqrt(np.sum([(u - v) ** 2 for u, v in zip(u_vector, v_vector)]))\n",
    "    return distance\n",
    "\n",
    "\n",
    "# Given a tweet, return the scores of the most similar/dissimilar pairs of words\n",
    "def get_similarity_measures(tweet, vec_map, weighted=False, verbose=True):\n",
    "    # Filter a bit the tweet so that no punctuation and no stopwords are included\n",
    "    stopwords = data_proc.get_stopwords_list()\n",
    "    filtered_tweet = list(set([w.lower() for w in tweet.split()\n",
    "                               if w.isalnum() and w not in stopwords and w.lower() in vec_map.keys()]))\n",
    "    # Compute similarity scores between any 2 words in filtered tweet\n",
    "    similarity_scores = []\n",
    "    max_words = []\n",
    "    min_words = []\n",
    "    max_score = -100\n",
    "    min_score = 100\n",
    "    for i in range(len(filtered_tweet) - 1):\n",
    "        wi = filtered_tweet[i]\n",
    "        for j in range(i + 1, len(filtered_tweet)):\n",
    "            wj = filtered_tweet[j]\n",
    "            similarity = cosine_similarity(vec_map[wi], vec_map[wj])\n",
    "            if weighted:\n",
    "                similarity /= euclidean_distance(vec_map[wi], vec_map[wj])\n",
    "            similarity_scores.append(similarity)\n",
    "            if max_score < similarity:\n",
    "                max_score = similarity\n",
    "                max_words = [wi, wj]\n",
    "            if min_score > similarity:\n",
    "                min_score = similarity\n",
    "                min_words = [wi, wj]\n",
    "    if verbose:\n",
    "        print(\"Filtered tweet: \", filtered_tweet)\n",
    "        if max_score != -100:\n",
    "            print(\"Maximum similarity is \", max_score, \" between words \", max_words)\n",
    "        else:\n",
    "            print(\"No max! Scores are: \", similarity_scores)\n",
    "        if min_score != 100:\n",
    "            print(\"Minimum similarity is \", min_score, \" between words \", min_words)\n",
    "        else:\n",
    "            print(\"No min! Scores are: \", similarity_scores)\n",
    "    return max_score, min_score\n",
    "\n",
    "\n",
    "# Custom metric function adjusted from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Recall metric. Only computes a batch-wise average of recall,\n",
    "    # a metric for multi-label classification of how many relevant items are selected.\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    # Precision metric. Only computes a batch-wise average of precision,\n",
    "    # a metric for multi-label classification of how many selected items are relevant.\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision*recall) / (precision+recall))\n",
    "\n",
    "\n",
    "# This code allows you to see the mislabelled examples\n",
    "def analyse_mislabelled_examples(x_test, y_test, y_pred):\n",
    "    for i in range(len(y_test)):\n",
    "        num = np.argmax(y_pred[i])\n",
    "        if num != y_test[i]:\n",
    "            print('Expected:', y_test[i], ' but predicted ', num)\n",
    "            print(x_test[i])\n",
    "# Extract each tweet's emojis - obv. it's just a brute force solution (so, it's slow) but works in ALL cases\n",
    "def extract_emojis(tweets):\n",
    "    emojis = []\n",
    "    for tw in tweets:\n",
    "        tw_emojis = []\n",
    "        for word in tw:\n",
    "            chars = list(word)\n",
    "            for ch in chars:\n",
    "                if ch in emoji.UNICODE_EMOJI:\n",
    "                    tw_emojis.append(ch)\n",
    "        emojis.append(' '.join(tw_emojis))\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwhvrY-FQVEA"
   },
   "outputs": [],
   "source": [
    "# Calculate the variance of an embedding (like glove, word2vec, emoji2vec, etc)\n",
    "# Used to sample new uniform distributions of vectors in the interval [-variance, variance]\n",
    "def embedding_variance(vec_map):\n",
    "    variance = np.sum([np.var(vec) for vec in vec_map.vectors]) / len(vec_map.vectors)\n",
    "    return variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdDRmgcdXj1m"
   },
   "outputs": [],
   "source": [
    "# Get some idea about the max and mean length of the tweets (useful for deciding on the sequence length)\n",
    "def get_max_len_info(tweets, average=False):\n",
    "    sum_of_length = sum([len(l.split()) for l in tweets])\n",
    "    avg_tweet_len = sum_of_length / float(len(tweets))\n",
    "    print(\"Mean of train tweets: \", avg_tweet_len)\n",
    "    max_tweet_len = len(max(tweets, key=len).split())\n",
    "    print(\"Max tweet length is = \", max_tweet_len)\n",
    "    if average:\n",
    "        return avg_tweet_len\n",
    "    return max_tweet_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJLMqGyHXuH7"
   },
   "outputs": [],
   "source": [
    "def encode_text_as_word_indexes(train_tweets, test_tweets , max_num_words=None, lower=False, char_level=False):\n",
    "    # Create the tokenizer\n",
    "    tokenizer = Tokenizer(num_words=max_num_words, filters='', lower=lower, split=\" \", char_level=char_level)\n",
    "    # Fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_tweets)\n",
    "    # Encode each example as a sequence of word indexes based on the vocabulary of the tokenizer\n",
    "    x_train = tokenizer.texts_to_sequences(train_tweets)\n",
    "    x_test=tokenizer.texts_to_sequences(test_tweets)\n",
    "   \n",
    "    return tokenizer, x_train,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38NTuvSyHfqd"
   },
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    emoji___=''.join(c for c in text if c in emoji.UNICODE_EMOJI)\n",
    "    return emoji.demojize(emoji___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SfdnqVQHiEl"
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZW4cg-xHvY1"
   },
   "outputs": [],
   "source": [
    "def contain_emoji(tweets):\n",
    "\n",
    "  emoji___=''.join(c for c in tweets if c in emoji.UNICODE_EMOJI)\n",
    "  # print(emoji___)\n",
    "  # print(tweets)\n",
    "  if len(emoji___)>0:\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \n",
    "    df[\"word_count\"] = df[\"cleaned_text\"].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    df[\"contain_emoji\"] = False\n",
    "    \n",
    "    df[\"demoji_text\"] = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "      # df.loc[index, 'cleaned_text']=remove_urls(row[\"cleaned_text\"])\n",
    "      if contain_emoji(row[\"tweet\"]):\n",
    "        df.loc[index, 'contain_emoji']=True\n",
    "        df.loc[index, 'demoji_text']=convert_emojis(row[\"tweet\"])\n",
    "        # print(convert_emojis(row[\"cleaned_text\"]))\n",
    "\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC4pF5tqpsfz"
   },
   "outputs": [],
   "source": [
    "# Compute the word-embedding matrix\n",
    "def get_embedding_matrix(word2vec_map, word_to_index, embedding_dim, init_unk=True, variance=None):\n",
    "    # Get the variance of the embedding map\n",
    "    if init_unk and variance is None:\n",
    "        variance = embedding_variance(word2vec_map)\n",
    "        print(\"Word vectors have variance \", variance)\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors)\n",
    "    embedding_matrix = np.zeros((len(word_to_index) + 1, embedding_dim))\n",
    "    for word, i in word_to_index.items():\n",
    "        embedding_vector=None \n",
    "        if word in word2vec_map.wv:\n",
    "          embedding_vector = word2vec_map[word]\n",
    "\n",
    "        # embedding_vector = word2vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif init_unk:\n",
    "            # Unknown tokens are initialized randomly by sampling from a uniform distribution [-var, var]\n",
    "            seed(1337603)\n",
    "            embedding_matrix[i] = np.random.uniform(-variance, variance, size=(1, embedding_dim))\n",
    "        # else:\n",
    "        #    print(\"Not found: \", word)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xltj3tPwQrRd"
   },
   "outputs": [],
   "source": [
    "def keras_dl( word2vec_embegging,emoji_embedd,voacb_shape_embdd,input_shape_1 ):   \n",
    "    inp = Input(shape = (input_shape_1,), name = 'text_clean')\n",
    "    x = Embedding(voacb_shape_embdd,300,weights = [word2vec_embegging], trainable = False)(inp)\n",
    "    # x1 = SpatialDropout1D(0.5)(x)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
    "    x_lstm_c1d = Conv1D(64,kernel_size=3,padding='valid',activation='tanh')(x_lstm)\n",
    "    x_lstm_c1d_gp = GlobalMaxPooling1D()(x_lstm_c1d)\n",
    "    \n",
    "    x_gru = Bidirectional(GRU(128, return_sequences = True))(x)\n",
    "    x_gru_c1d = Conv1D(64,kernel_size=2,padding='valid',activation='tanh')(x_gru)\n",
    "    x_gru_c1d_gp = GlobalMaxPooling1D()(x_gru_c1d)\n",
    "\n",
    "    inp2 = Input(shape = (input_shape_1,), name = 'emoji')\n",
    "    x2 = Embedding(voacb_shape_embdd,300,weights = [emoji_embedd], trainable = False)(inp2)\n",
    "    X2_conv1 = Conv1D(64, 5, kernel_initializer='he_normal', padding='valid', activation='relu')(x2)\n",
    "    X2_lstm = LSTM(16, kernel_initializer='he_normal', activation='tanh',\n",
    "             dropout=0.5, return_sequences=True)(X2_conv1)\n",
    "    x2_flatten = Flatten()(X2_lstm)\n",
    "    x2_dense =(Dense(32, activation='tanh') (x2_flatten))  \n",
    "    \n",
    "\n",
    "    \n",
    "    x_f = concatenate([x_lstm_c1d_gp, x_gru_c1d_gp])\n",
    "    x_f = BatchNormalization()(x_f)\n",
    "    x_f =(Dense(128, activation='tanh') (x_f))    \n",
    "    x_f = BatchNormalization()(x_f)\n",
    "    x_f = concatenate([x_f, x2_dense])    \n",
    "    x_f = (Dense(64, activation='tanh') (x_f))\n",
    "    x_f = Dense(2, activation = \"sigmoid\")(x_f)\n",
    "    model = Model(inputs = [inp, inp2], outputs = x_f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra6LXxb35uv7"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fm34AVoVFRh1",
    "outputId": "2903788b-e408-4ef6-d42b-ccac69b8909c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq4sJk9UFVYj"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY4z4R1Mbfpe"
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/ArSarcasm_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "tin8cBWKbuLK",
    "outputId": "dc3ce2ef-631a-49ec-88a1-6d6346441b5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialect</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>original_sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>msa</td>\n",
       "      <td>True</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>\"@AbuEmad74241481 @Cesars2014 Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ...</td>\n",
       "      <td>semeval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gulf</td>\n",
       "      <td>False</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>\"RT @JannetForster: Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ù… ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ...</td>\n",
       "      <td>semeval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>msa</td>\n",
       "      <td>True</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†</td>\n",
       "      <td>astd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>msa</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>\"@EGYPTAIR Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§Ù‹ Ø§Ù†ÙŠ ÙÙŠ ...</td>\n",
       "      <td>semeval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>msa</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Ù…Ø§ Ù„Ø§ ØªØ±Ø§Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø´Ø­...</td>\n",
       "      <td>semeval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dialect  sarcasm  ...                                              tweet   source\n",
       "0     msa     True  ...  \"@AbuEmad74241481 @Cesars2014 Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ...  semeval\n",
       "1    gulf    False  ...  \"RT @JannetForster: Ø§Ù„Ø¨Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ù… ØµØ§Ù…Ùˆ Ø¨Ù‚ÙˆÙ„ÙƒÙ… ...  semeval\n",
       "2     msa     True  ...             Ø§Ø´Ø§Ø±Ø© Ø±Ø§Ø¨Ø¹Ø© Ø§Ø´Ø¨Ù‡ Ø¨Ù†Ø§Ø± ØªØ­Ø±Ù‚ Ø§Ù„Ø§Ù†Ù‚Ù„Ø§Ø¨ÙŠÙŠÙ†     astd\n",
       "3     msa    False  ...  \"@EGYPTAIR Ù…Ø§Ù‡ÙŠ Ù…Ù…ÙŠØ²Ø§Øª Ø¯Ø±Ø¬Ù‡ Ø¨Ø²Ù†Ø³ Ø¹Ù„Ù…Ø§Ù‹ Ø§Ù†ÙŠ ÙÙŠ ...  semeval\n",
       "4     msa    False  ...  Ù…Ø§ Ù„Ø§ ØªØ±Ø§Ù‡ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ„ÙØ§Ø² Ù…Ù†Ø§ÙØ³Ø© Ø´Ø¯ÙŠØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±Ø´Ø­...  semeval\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "JinxywYkYTq3",
    "outputId": "6de6eed3-10ac-46a7-def3-a6af2fbb9873"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>gulf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sarcasm sentiment dialect\n",
       "0  \"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...    False       NEU     msa\n",
       "1  \"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...    False       NEU     msa\n",
       "2  â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...     True       NEG     msa\n",
       "3  \"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...     True       NEG    gulf\n",
       "4  \"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...    False       NEU     msa"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CsBEDPlIBnK"
   },
   "outputs": [],
   "source": [
    "train_data['cleaned_text'] = train_data.tweet.apply(clean_text)\n",
    "test_data['cleaned_text'] = test_data.tweet.apply(clean_text)\n",
    "train_data['cleaned_text'] = train_data.cleaned_text.apply(remove_urls)\n",
    "test_data['cleaned_text'] = test_data.cleaned_text.apply(remove_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WydPRwkrHkjr"
   },
   "outputs": [],
   "source": [
    "train_data = transform(train_data)\n",
    "test_data = transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "e8AsvhoznKoR",
    "outputId": "0b898bf0-50c9-47c6-81b2-6f66145ef634"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>dialect</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>contain_emoji</th>\n",
       "      <th>demoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "      <td>Ø¯ Ù…Ø­Ù…ÙˆØ¯ Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠØ§Ø±ÙŠ Ø§Ù† Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ø­Ù…Ø¯ Ø´ÙÙŠÙ‚ Ø±Ù‚Ù… Ù…Ù‡...</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "      <td>Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§Ø§Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø±</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>:smiling_face_with_heart-eyes:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>msa</td>\n",
       "      <td>Ø§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø§ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ† ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† Ù„Ø§...</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>gulf</td>\n",
       "      <td>Ù…Ø³Ø§ÙƒÙŠÙ† Ù…Ù† Ø§Ù„ØµØ¨Ø­ ÙˆÙ‡ÙˆÙ…Ø§ Ø±Ø§ÙŠØ­ÙŠÙ† Ø±Ø§Ø¬Ø¹ÙŠÙ† Ø¹Ø§ÙŠ ØºÙˆØºÙ„ Øª...</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "      <td>Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠÙ‡ ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„Ø¨ ÙˆÙ„Ø§...</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>Ù‡Ù„Ø£ ØµØ§ÙŠØ± Ø§Ù†Øª ÙŠØ§ #ÙÙ„Ø¹ÙˆØ· Ø¨Ø¯Ùƒ ØªØ¹Ø·ÙŠ Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø¹ ØªÙˆÙŠØª...</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>levant</td>\n",
       "      <td>Ù‡Ù„Ø§ ØµØ§ÙŠØ± Ø§Ù†Øª ÙŠØ§ÙÙ„Ø¹ÙˆØ· Ø¨Ø¯Ùƒ ØªØ¹Ø·ÙŠ Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø¹ ØªÙˆÙŠØªØ± ...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12544</th>\n",
       "      <td>\"Ù„Ø§ Ø§Ù„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ù„Ù‡ğŸ’œ#Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ± https://t.c...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>egypt</td>\n",
       "      <td>Ù„Ø§ Ø§Ù„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ù„Ù‡Ø§ÙŠÙÙˆÙ† Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ±</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>:purple_heart:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12545</th>\n",
       "      <td>\"RT @turkyepost: #Ø£Ø±Ø¯ÙˆØºØ§Ù† : Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø±ØªÙƒØ¨ ØºÙŠ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NEU</td>\n",
       "      <td>msa</td>\n",
       "      <td>Ø§Ø±Ø¯ÙˆØºØ§Ù† Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø±ØªÙƒØ¨ ØºÙŠØ± Ù…Ø³Ù„Ù… Ù„Ø§ ÙŠØ³Ù…ÙˆÙ†Ù‡Ø§ Ø¹Ù…...</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12546</th>\n",
       "      <td>RT @Yousiif65: Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ±ØŸğŸ‘“ğŸ© https://t.co/959Oo...</td>\n",
       "      <td>False</td>\n",
       "      <td>POS</td>\n",
       "      <td>egypt</td>\n",
       "      <td>Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ±</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>:glasses::top_hat:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12547</th>\n",
       "      <td>Ø§Ù„Ù†Øª ÙŠÙ„Ø¹Ù† Ø´ÙƒÙ„Ù‡ Ø·ÙˆÙ„ Ø§Ù„ÙˆÙ‚Øª ØªÙ…Ø§Ù… Ù„ÙŠÙ† Ø¨Ø¯Ø§ Ø§Ù„Ø­ÙÙ„</td>\n",
       "      <td>True</td>\n",
       "      <td>NEG</td>\n",
       "      <td>egypt</td>\n",
       "      <td>Ø§Ù„Ù†Øª ÙŠÙ„Ø¹Ù† Ø´ÙƒÙ„Ù‡ Ø·ÙˆÙ„ Ø§Ù„ÙˆÙ‚Øª ØªÙ…Ø§Ù… Ù„ÙŠÙ† Ø¨Ø¯Ø§ Ø§Ù„Ø­ÙÙ„</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12548 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  ...                     demoji_text\n",
       "0      \"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...  ...                                \n",
       "1      \"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...  ...  :smiling_face_with_heart-eyes:\n",
       "2      â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...  ...                                \n",
       "3      \"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...  ...                                \n",
       "4      \"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...  ...                                \n",
       "...                                                  ...  ...                             ...\n",
       "12543  Ù‡Ù„Ø£ ØµØ§ÙŠØ± Ø§Ù†Øª ÙŠØ§ #ÙÙ„Ø¹ÙˆØ· Ø¨Ø¯Ùƒ ØªØ¹Ø·ÙŠ Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ø¹ ØªÙˆÙŠØª...  ...                                \n",
       "12544  \"Ù„Ø§ Ø§Ù„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ù„Ù‡ğŸ’œ#Ø£ÙŠÙÙˆÙ†_Ø§Ù„Ø¨Ø±ÙˆÙÙŠØ³ÙˆØ± https://t.c...  ...                  :purple_heart:\n",
       "12545  \"RT @turkyepost: #Ø£Ø±Ø¯ÙˆØºØ§Ù† : Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø±ØªÙƒØ¨ ØºÙŠ...  ...                                \n",
       "12546  RT @Yousiif65: Ù‡Ø§Ø±ÙŠ Ø¨ÙˆØªØ±ØŸğŸ‘“ğŸ© https://t.co/959Oo...  ...              :glasses::top_hat:\n",
       "12547        Ø§Ù„Ù†Øª ÙŠÙ„Ø¹Ù† Ø´ÙƒÙ„Ù‡ Ø·ÙˆÙ„ Ø§Ù„ÙˆÙ‚Øª ØªÙ…Ø§Ù… Ù„ÙŠÙ† Ø¨Ø¯Ø§ Ø§Ù„Ø­ÙÙ„  ...                                \n",
       "\n",
       "[12548 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkGS75X9YAHf"
   },
   "outputs": [],
   "source": [
    "# Transform the output into categorical data or just keep it as it is (in a numpy array)\n",
    "train_labels_task1 = to_categorical(np.asarray(train_data['sarcasm']))\n",
    "# train_labels_task2 = to_categorical(np.asarray(train_data['sentiment'].factorize()[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrVWLKQyda3F"
   },
   "outputs": [],
   "source": [
    "# Transform the output into categorical data or just keep it as it is (in a numpy array)\n",
    "test_labels_task1 = to_categorical(np.asarray(test_data['sarcasm']))\n",
    "# test_labels_task2 = to_categorical(np.asarray(test_data['sentiment'].factorize()[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2DowlPQISV0"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300 #176 # based on our inputs; TODO: remove outliers? dynamically calculate!\n",
    "MAX_NB_WORDS = len(word2vec_map.vocab)\n",
    "EMBEDDING_DIM = 300 # w2v, fastText; GloVe=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt4MCMAfIc6l"
   },
   "outputs": [],
   "source": [
    "word2vec_map = load_w2v(\"/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/arabic-news.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CPPtHHeJE8T"
   },
   "outputs": [],
   "source": [
    "emoji2vec_map = load_w2v(\"/content/drive/MyDrive/Sentiment_Analysis_Arabic/Dataset/emoji2vec.bin\",True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Data  vector2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAIwvumPSb73",
    "outputId": "60d68539-55ba-428a-c3ea-2cfd897fc7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45466 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Get the max length of the train tweets\n",
    "max_tweet_length = train_data.cleaned_text.map(len).max()\n",
    "\n",
    "# Convert all tweets into sequences of word indices\n",
    "tokenizer, train_indices,test_indices = encode_text_as_word_indexes(train_data['cleaned_text'],test_data['cleaned_text'],  lower=True)\n",
    "vocab_size = len(tokenizer.word_counts) + 1\n",
    "word_to_index = tokenizer.word_index\n",
    "print(\"There are %s unique tokens.\" % len(word_to_index))\n",
    "\n",
    "# Pad sequences with 0s (can do it post or pre - post works better here)\n",
    "x_train = pad_sequences(train_indices, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "x_test = pad_sequences(test_indices, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "\n",
    "# Get the word to index and the index to word mappings\n",
    "word_index = tokenizer.word_index\n",
    "index_to_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Set the sequence length\n",
    "SEQUENCE_LENGTH = max_tweet_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1_Ko30QH9f0",
    "outputId": "c2a74b57-6f81-417e-de5f-8e11642c1ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 799 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Convert all tweets into sequences of word indices\n",
    "tokenizer_emji, train_indices_emji,test_indices_emji = encode_text_as_word_indexes(train_data['demoji_text'],test_data['demoji_text'],  lower=True)\n",
    "word_to_index_emji= tokenizer_emji.word_index\n",
    "print(\"There are %s unique tokens.\" % len(word_to_index_emji))\n",
    "\n",
    "# Pad sequences with 0s (can do it post or pre - post works better here)\n",
    "x_train_emji = pad_sequences(train_indices_emji, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "x_test_emji = pad_sequences(test_indices_emji, maxlen=max_tweet_length, padding=\"post\", truncating=\"post\", value=0.)\n",
    "\n",
    "\n",
    "\n",
    "# Get the word to index and the index to word mappings\n",
    "word_index_emji = tokenizer_emji.word_index\n",
    "index_to_word_emji = {index: word for word, index in word_index_emji.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyr-oqh_Zy4d"
   },
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlAt8UjEs3uU",
    "outputId": "725c845e-7d01-4870-90cf-b5b50005fff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors have variance  0.7012621721375845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "w2v_embedd=get_embedding_matrix(word2vec_map,word_index,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKYStjiltQ5D",
    "outputId": "d4c6c6ec-a030-492a-df6d-baeec1b11cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors have variance  0.0033188830794231637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "emoji_embedd=get_embedding_matrix(emoji2vec_map,word_index,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWZNylYdRAAw"
   },
   "outputs": [],
   "source": [
    "model=keras_dl(w2v_embedd,emoji_embedd,w2v_embedd.shape[0],max_tweet_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "602Y9_63uar4"
   },
   "outputs": [],
   "source": [
    "my_optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.99, decay=0.01)\n",
    "reduceLR = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wgDRutXsoUb"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=my_optimizer, metrics=['accuracy',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKOiSdq_xeo2",
    "outputId": "c7362d1c-3cad-45d1-99ca-dab98ba22559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "353/353 [==============================] - 40s 88ms/step - loss: 0.5837 - accuracy: 0.7112 - f1_m: 0.6771 - precision_m: 0.6795 - recall_m: 0.6766 - val_loss: 0.4450 - val_accuracy: 0.8112 - val_f1_m: 0.7780 - val_precision_m: 0.8002 - val_recall_m: 0.7596\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 29s 81ms/step - loss: 0.3686 - accuracy: 0.8437 - f1_m: 0.8096 - precision_m: 0.8148 - recall_m: 0.8059 - val_loss: 0.4424 - val_accuracy: 0.8096 - val_f1_m: 0.7695 - val_precision_m: 0.7868 - val_recall_m: 0.7565\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 29s 82ms/step - loss: 0.3412 - accuracy: 0.8583 - f1_m: 0.8240 - precision_m: 0.8283 - recall_m: 0.8212 - val_loss: 0.4407 - val_accuracy: 0.8112 - val_f1_m: 0.7747 - val_precision_m: 0.7916 - val_recall_m: 0.7619\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 28s 81ms/step - loss: 0.3258 - accuracy: 0.8640 - f1_m: 0.8316 - precision_m: 0.8350 - recall_m: 0.8298 - val_loss: 0.4399 - val_accuracy: 0.8080 - val_f1_m: 0.7760 - val_precision_m: 0.7924 - val_recall_m: 0.7635\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 29s 81ms/step - loss: 0.3110 - accuracy: 0.8681 - f1_m: 0.8336 - precision_m: 0.8400 - recall_m: 0.8291 - val_loss: 0.4399 - val_accuracy: 0.8096 - val_f1_m: 0.7759 - val_precision_m: 0.7911 - val_recall_m: 0.7643\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 29s 81ms/step - loss: 0.2984 - accuracy: 0.8755 - f1_m: 0.8394 - precision_m: 0.8432 - recall_m: 0.8375 - val_loss: 0.4400 - val_accuracy: 0.8080 - val_f1_m: 0.7785 - val_precision_m: 0.7940 - val_recall_m: 0.7666\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 29s 81ms/step - loss: 0.2934 - accuracy: 0.8788 - f1_m: 0.8462 - precision_m: 0.8502 - recall_m: 0.8437 - val_loss: 0.4402 - val_accuracy: 0.8056 - val_f1_m: 0.7792 - val_precision_m: 0.7935 - val_recall_m: 0.7682\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.2848 - accuracy: 0.8803 - f1_m: 0.8521 - precision_m: 0.8546 - recall_m: 0.8511 - val_loss: 0.4399 - val_accuracy: 0.8088 - val_f1_m: 0.7799 - val_precision_m: 0.7952 - val_recall_m: 0.7682\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 28s 81ms/step - loss: 0.2849 - accuracy: 0.8837 - f1_m: 0.8477 - precision_m: 0.8516 - recall_m: 0.8457 - val_loss: 0.4397 - val_accuracy: 0.8127 - val_f1_m: 0.7792 - val_precision_m: 0.7939 - val_recall_m: 0.7682\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 29s 82ms/step - loss: 0.2629 - accuracy: 0.8962 - f1_m: 0.8616 - precision_m: 0.8612 - recall_m: 0.8635 - val_loss: 0.4401 - val_accuracy: 0.8104 - val_f1_m: 0.7783 - val_precision_m: 0.7905 - val_recall_m: 0.7698\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training data\n",
    "history = model.fit([x_train,x_train_emji], train_labels_task1, batch_size=32, epochs=100, shuffle=True,\n",
    "                             validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ckFlfqAU17q",
    "outputId": "b59753aa-869e-4379-bd31-50b7f45f14ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 33ms/step - loss: 0.1486 - accuracy: 0.9578 - f1_m: 0.9348 - precision_m: 0.9352 - recall_m: 0.9356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14855192601680756,\n",
       " 0.9578198790550232,\n",
       " 0.9348338842391968,\n",
       " 0.9352262020111084,\n",
       " 0.9355745315551758]"
      ]
     },
     "execution_count": 334,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x_test,x_test_emji],test_labels_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict([x_test,x_test_emji])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsorted_y = np.argsort(predictions)[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels_task1.argmax(axis=1), argsorted_y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "k8gpz2mVr1a4"
   ],
   "name": "Sarcasm-Detection-Multiple-DL-approach-task1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
